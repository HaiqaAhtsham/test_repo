{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  is_sarcastic  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
      "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
      "\n",
      "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
      "0         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "1         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "2         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "3         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "4         NaN         NaN         NaN         NaN        NaN         NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (replace 'urdu_dataset.csv' with your actual dataset file)\n",
    "# The dataset contains two columns: 'text' for the posts and 'sentiment' for the sentiment labels.\n",
    "df = pd.read_csv('urdu_sarcastic_dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['urdu_text', 'is_sarcastic', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4',\n",
       "       'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  is_sarcastic  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
      "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
      "\n",
      "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \\\n",
      "0         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "3         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...  \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº ...  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
      "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜  \n",
      "4  `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’ ...  \n"
     ]
    }
   ],
   "source": [
    "#Sopword removal\n",
    "# Custom list of Urdu stopwords\n",
    "stopwords = [\"Ø§ÙˆØ±\", \"ÛŒÛ\", \"Ú©Û\", \"ÛÛŒÚº\", \"ØªÚ¾Ø§\", \"ØªÚ¾ÛŒ\", \"Ù…ÛŒÚº\", \"Ø³Û’\", \"Ú©Ø§\", \"Ú©Ùˆ\"]  # Add more stopwords as needed\n",
    "sentiment_carrying_stopwords = [\"Ù†ÛÛŒÚº\", \"Ø¨Ø±Ø§\"]  # Stopwords with sentiment\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text, stopwords, sentiment_carrying_stopwords):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords or word in sentiment_carrying_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the dataset\n",
    "df['cleaned_text'] = df['urdu_text'].apply(lambda x: remove_stopwords(str(x), stopwords, sentiment_carrying_stopwords))\n",
    "\n",
    "# Display the dataset after stopword removal\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  is_sarcastic  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
      "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
      "\n",
      "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \\\n",
      "0         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "3         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0   ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº ...  \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
      "3                                        Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†   \n",
      "4   Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’  Ø­...  \n"
     ]
    }
   ],
   "source": [
    "#Remove Punctuation, Emojis, URLs, and Hashtags\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Emoji to sentiment dictionary\n",
    "emoji_sentiment = {\n",
    "    \"ğŸ˜Š\": \"positive\",\n",
    "    \"ğŸ˜¢\": \"negative\",\n",
    "    \"â¤\": \"positive\",\n",
    "    \"ğŸ˜ \": \"negative\",\n",
    "    \"ğŸ˜‚\": \"positive\",    \n",
    "    \"ğŸ˜\": \"positive\",  \n",
    "}\n",
    "\n",
    "# Function to clean text: remove URLs, hashtags, mentions, and punctuation\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'#\\S+|@\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to replace emojis with sentiment\n",
    "def replace_emojis_with_sentiment(text):\n",
    "    return emoji.replace_emoji(text, replace=lambda e: emoji_sentiment.get(e, ''))\n",
    "\n",
    "# Combine cleaning functions\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = replace_emojis_with_sentiment(text)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# Display the dataset after text cleaning\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  is_sarcastic  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
      "5        Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥           1.0   \n",
      "\n",
      "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \\\n",
      "0         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "5         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0   ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº ...  \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
      "4   Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’  Ø­...  \n",
      "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’   \n"
     ]
    }
   ],
   "source": [
    "# Filter Short Conversations\n",
    "# Function to filter out short posts\n",
    "def filter_short_posts(text, min_word_count=3):\n",
    "    word_count = len(text.split())\n",
    "    if word_count < min_word_count:\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "# Apply the function to the dataset\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: filter_short_posts(x))\n",
    "\n",
    "# Drop rows where posts were too short (None values)\n",
    "df = df.dropna(subset=['cleaned_text'])\n",
    "\n",
    "# Display the dataset after filtering short posts\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  is_sarcastic  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
      "5        Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥           1.0   \n",
      "\n",
      "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \\\n",
      "0         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "5         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0   ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº ...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
      "4   Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’  Ø­...   \n",
      "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’    \n",
      "\n",
      "                                        stemmed_text  \n",
      "0          ÛÙˆ Ù„ÛŒÙ† Ø¯ Ù…ÛŒØ± Ø´Ø§Ø¯ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Û Ú©ÙˆØ¬ Ù†Û Ú†Ø§ÛÛŒ  \n",
      "1           Ú†Ù„ Ù…ÛÙ…Ø§Ù† Ú©Ú¾Ø§Ù† Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú† Ù† Ø¯Ø³Ø¯ Ø¢Úº Ù…  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú© Ø¯Ù† Ø¨Ú¾Ø±ÛŒ Ø²Ù… Ø¯Ø§Ø± Ù„Ú¯Ø§Ø¦ Ú¯Ø¦ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† ...  \n",
      "4        Ù…Ø±Ø§Ø¯ Ø¹Ù„ Ø´Ø§ Ú© Ø¨Ú¾ÛŒØ³ Úˆ Ø¬ Ø¢Ø¦ Ø§ÛŒØ³ Ø¢Ø¦ ØªÚ¾ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  \n",
      "5                 Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØª  \n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "# A simple list of common Urdu suffixes for gender and plurality\n",
    "suffixes = ['ÛŒ', 'Û’', 'ÙˆÚº', 'ÛŒÚº', 'Ø§', 'Û']\n",
    "\n",
    "# Function for stemming in Urdu\n",
    "def urdu_stemmer(word):\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word  # Return the word if no suffix matches\n",
    "\n",
    "# Stem all words in a sentence\n",
    "def stem_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    stemmed_words = [urdu_stemmer(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Apply stemming to the dataset\n",
    "df['stemmed_text'] = df['cleaned_text'].apply(lambda x: stem_sentence(str(x)))\n",
    "\n",
    "# Display the dataset after stemming\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  is_sarcastic  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
      "5        Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥           1.0   \n",
      "\n",
      "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \\\n",
      "0         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "5         NaN         NaN         NaN         NaN        NaN         NaN   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0   ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº ...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
      "4   Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’  Ø­...   \n",
      "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’    \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0          ÛÙˆ Ù„ÛŒÙ† Ø¯ Ù…ÛŒØ± Ø´Ø§Ø¯ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Û Ú©ÙˆØ¬ Ù†Û Ú†Ø§ÛÛŒ   \n",
      "1           Ú†Ù„ Ù…ÛÙ…Ø§Ù† Ú©Ú¾Ø§Ù† Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú† Ù† Ø¯Ø³Ø¯ Ø¢Úº Ù…   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú© Ø¯Ù† Ø¨Ú¾Ø±ÛŒ Ø²Ù… Ø¯Ø§Ø± Ù„Ú¯Ø§Ø¦ Ú¯Ø¦ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† ...   \n",
      "4        Ù…Ø±Ø§Ø¯ Ø¹Ù„ Ø´Ø§ Ú© Ø¨Ú¾ÛŒØ³ Úˆ Ø¬ Ø¢Ø¦ Ø§ÛŒØ³ Ø¢Ø¦ ØªÚ¾ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
      "5                 Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØª   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0             ÛÙˆ Ù„ÛŒÙ† Ø¯ Ù…ÛŒØ± Ø´Ø§Ø¯ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ©  Ú©ÙˆØ¬ Ù† Ú†Ø§Û  \n",
      "1           Ú†Ù„ Ù…ÛÙ…Ø§Ù† Ú©Ú¾Ø§Ù† Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú† Ù† Ø¯Ø³Ø¯ Ø¢Úº Ù…  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú© Ø¯Ù† Ø¨Ú¾Ø± Ø²Ù… Ø¯Ø§Ø± Ù„Ú¯Ø§Ø¦ Ú¯Ø¦ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©...  \n",
      "4         Ù…Ø±Ø§Ø¯ Ø¹Ù„ Ø´ Ú© Ø¨Ú¾ÛŒØ³ Úˆ Ø¬ Ø¢Ø¦ Ø§ÛŒØ³ Ø¢Ø¦ ØªÚ¾ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  \n",
      "5                  Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø±  Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØª  \n"
     ]
    }
   ],
   "source": [
    "#try lemmatization\n",
    "# Simple dictionary for lemmatization (expand as needed)\n",
    "lemma_dict = {\n",
    "    \"Ú†Ù„ Ø±ÛÛŒ\": \"Ú†Ù„\",  # is moving -> move\n",
    "    \"Ú©Ú¾ÛŒÙ„Ø§\": \"Ú©Ú¾ÛŒÙ„\",  # played -> play\n",
    "    \"Ù„Ú‘Ú©ÙˆÚº\": \"Ù„Ú‘Ú©Ø§\",  # boys -> boy\n",
    "    \"Ø§Ú†Ú¾ÛŒ\": \"Ø§Ú†Ú¾Ø§\",  # good (feminine) -> good\n",
    "}\n",
    "\n",
    "# Function for lemmatization in Urdu\n",
    "def urdu_lemmatizer(word):\n",
    "    # Check if the word exists in the lemma dictionary\n",
    "    if word in lemma_dict:\n",
    "        return lemma_dict[word]\n",
    "    \n",
    "    # Fallback to stemming if not found in the dictionary\n",
    "    return urdu_stemmer(word)\n",
    "\n",
    "# Lemmatize all words in a sentence\n",
    "def lemmatize_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    lemmatized_words = [urdu_lemmatizer(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to the dataset\n",
    "df['lemmatized_text'] = df['stemmed_text'].apply(lambda x: lemmatize_sentence(str(x)))\n",
    "\n",
    "# Display the final dataset with lemmatized text\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Phase3- FEATURE EXTRACTION----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to c:\\Users\\haiqa\\OneDrive\\Deskt\n",
      "[nltk_data]     op\\NLP\\assignment_1\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Set the NLTK data path to your custom directory\n",
    "nltk.data.path.append(os.path.join(os.getcwd(), 'nltk_data'))\n",
    "\n",
    "# Download the Punkt tokenizer model\n",
    "nltk.download('punkt', download_dir=os.path.join(os.getcwd(), 'nltk_data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     lemmatized_text  \\\n",
      "0             ÛÙˆ Ù„ÛŒÙ† Ø¯ Ù…ÛŒØ± Ø´Ø§Ø¯ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ©  Ú©ÙˆØ¬ Ù† Ú†Ø§Û   \n",
      "1           Ú†Ù„ Ù…ÛÙ…Ø§Ù† Ú©Ú¾Ø§Ù† Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú† Ù† Ø¯Ø³Ø¯ Ø¢Úº Ù…   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú© Ø¯Ù† Ø¨Ú¾Ø± Ø²Ù… Ø¯Ø§Ø± Ù„Ú¯Ø§Ø¦ Ú¯Ø¦ Ø§Ù¾ÙˆØ²ÛŒØ´Ù† Ú©...   \n",
      "4         Ù…Ø±Ø§Ø¯ Ø¹Ù„ Ø´ Ú© Ø¨Ú¾ÛŒØ³ Úˆ Ø¬ Ø¢Ø¦ Ø§ÛŒØ³ Ø¢Ø¦ ØªÚ¾ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
      "5                  Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø±  Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØª   \n",
      "\n",
      "                           tokenized_lemmatized_text  \n",
      "0   [ÛÙˆ, Ù„ÛŒÙ†, Ø¯, Ù…ÛŒØ±, Ø´Ø§Ø¯, ÙØ³Ø§Ø¯Ù†, Ù¹Ú¾ÛŒÚ©, Ú©ÙˆØ¬, Ù†, Ú†Ø§Û]  \n",
      "1  [Ú†Ù„, Ù…ÛÙ…Ø§Ù†, Ú©Ú¾Ø§Ù†, Ø³Ø±Ùˆ, Ú©Ø±, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†, Ù†, Ø¯Ø³Ø¯, ...  \n",
      "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©, Ø¯Ù†, Ø¨Ú¾Ø±, Ø²Ù…, Ø¯Ø§Ø±, Ù„Ú¯Ø§Ø¦, Ú¯Ø¦,...  \n",
      "4  [Ù…Ø±Ø§Ø¯, Ø¹Ù„, Ø´, Ú©, Ø¨Ú¾ÛŒØ³, Úˆ, Ø¬, Ø¢Ø¦, Ø§ÛŒØ³, Ø¢Ø¦, ØªÚ¾, ...  \n",
      "5            [Ù‚Ø§Ø¨Ù„, Ø§Ø¹ØªØ¨Ø§Ø±, Ø§Ú©Ø«Ø±, Ù‚Ø§ØªÙ„, Ø§Ø¹ØªØ¨Ø§Ø±, ÛÙˆØª]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Simple regex-based tokenizer\n",
    "def simple_tokenize_urdu(text):\n",
    "    tokens = re.findall(r'\\S+', text)  # Match any non-whitespace character sequences\n",
    "    return tokens\n",
    "\n",
    "# Apply simple tokenization on the lemmatized text\n",
    "df['tokenized_lemmatized_text'] = df['lemmatized_text'].apply(simple_tokenize_urdu)\n",
    "\n",
    "# Display tokenized versions of several lemmatized texts\n",
    "print(df[['lemmatized_text', 'tokenized_lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with highest TF-IDF scores:\n",
      "     tfidf_score\n",
      "ØªÙˆ    528.682963\n",
      "Ú©Ø±    512.696197\n",
      "Ø¨Ú¾    465.945567\n",
      "ÛÙˆ    411.590584\n",
      "Ø§Ø³    345.438345\n",
      "Ù¾Ø±    317.888692\n",
      "Ø¢Ù¾    314.388385\n",
      "Ù…ÛŒØ±   252.560996\n",
      "ÙˆØ§Ù„   250.086957\n",
      "Ø§ÛŒÚ©   242.124841\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your dataset (assuming df is already defined with 'tokenized_lemmatized_text')\n",
    "# df = pd.read_csv('path_to_your_dataset.csv')\n",
    "\n",
    "# Join the tokenized text back into a single string for vectorization\n",
    "df['joined_lemmatized_text'] = df['tokenized_lemmatized_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['joined_lemmatized_text'])\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each word and convert to a 1D array\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "# Create a DataFrame to display the scores\n",
    "tfidf_df = pd.DataFrame(tfidf_scores, index=feature_names, columns=['tfidf_score']).sort_values(by='tfidf_score', ascending=False)\n",
    "\n",
    "# Get the top 10 words\n",
    "top_10_words_tfidf = tfidf_df.head(10)\n",
    "print(\"Top 10 words with highest TF-IDF scores:\")\n",
    "print(top_10_words_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words most similar to 'Ø§Ú†Ú¾Ø§':\n",
      "ØªÚ¾Ø§Ø§Ú†Ú¾: 0.5756\n",
      "68: 0.5720\n",
      "Ø¨ØªØ±: 0.5509\n",
      "Ø¨Ø±Ø³Ø§Ø¦: 0.5484\n",
      "Ø¯Ø¦ÙŠ: 0.5479\n"
     ]
    }
   ],
   "source": [
    "#Train Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Ensure that your tokenized lemmatized text is in the correct format for Word2Vec\n",
    "sentences = df['tokenized_lemmatized_text'].tolist()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Find top 5 words most similar to \"Ø§Ú†Ú¾Ø§\"\n",
    "similar_words = word2vec_model.wv.most_similar(\"Ø§Ú†Ú¾Ø§\", topn=5)\n",
    "print(\"Top 5 words most similar to 'Ø§Ú†Ú¾Ø§':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------PHASE 4------------------\n",
    "\n",
    "# If you have a column with lemmatized text, use that; otherwise, ensure your text is tokenized correctly.\n",
    "df['joined_lemmatized_text'] = df['tokenized_lemmatized_text'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('path_to_your_dataset.csv')\n",
    "\n",
    "# If you have a column with lemmatized text, use that; otherwise, ensure your text is tokenized correctly.\n",
    "df['joined_lemmatized_text'] = df['tokenized_lemmatized_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Tokenization function using NLTK\n",
    "def tokenize_urdu(text):\n",
    "    return word_tokenize(text)  # Use NLTK's word_tokenize\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = tokenize_urdu(text)\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Create empty counters for unigrams, bigrams, and trigrams\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\haiqa/nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\haiqa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\OneDrive\\\\Desktop\\\\NLP\\\\assignment_1\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate through each text entry in the DataFrame\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoined_lemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     unigrams \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     bigrams \u001b[38;5;241m=\u001b[39m generate_ngrams(text, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m     trigrams \u001b[38;5;241m=\u001b[39m generate_ngrams(text, \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[70], line 19\u001b[0m, in \u001b[0;36mgenerate_ngrams\u001b[1;34m(text, n)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_ngrams\u001b[39m(text, n):\n\u001b[1;32m---> 19\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_urdu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(ngrams(tokens, n))\n",
      "Cell \u001b[1;32mIn[70], line 15\u001b[0m, in \u001b[0;36mtokenize_urdu\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_urdu\u001b[39m(text):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haiqa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\haiqa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\haiqa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haiqa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haiqa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\haiqa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\haiqa/nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\haiqa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'c:\\\\Users\\\\haiqa\\\\OneDrive\\\\Desktop\\\\NLP\\\\assignment_1\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterate through each text entry in the DataFrame\n",
    "for text in df['joined_lemmatized_text']:\n",
    "    unigrams = generate_ngrams(text, 1)\n",
    "    bigrams = generate_ngrams(text, 2)\n",
    "    trigrams = generate_ngrams(text, 3)\n",
    "    \n",
    "    unigram_counter.update(unigrams)\n",
    "    bigram_counter.update(bigrams)\n",
    "    trigram_counter.update(trigrams)\n",
    "\n",
    "# Get the most common unigrams, bigrams, and trigrams\n",
    "top_10_unigrams = unigram_counter.most_common(10)\n",
    "top_10_bigrams = bigram_counter.most_common(10)\n",
    "top_10_trigrams = trigram_counter.most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Unigrams:\")\n",
    "print(top_10_unigrams)\n",
    "print(\"\\nTop 10 Bigrams:\")\n",
    "print(top_10_bigrams)\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "print(top_10_trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
